{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["simpleData = ((\"James\", \"Sales\", 3000), \\\n", "    (\"Michael\", \"Sales\", 4600),  \\\n", "    (\"Robert\", \"Sales\", 4100),   \\\n", "    (\"Maria\", \"Finance\", 3000),  \\\n", "    (\"James\", \"Sales\", 3000),    \\\n", "    (\"Scott\", \"Finance\", 3300),  \\\n", "    (\"Jen\", \"Finance\", 3900),    \\\n", "    (\"Jeff\", \"Marketing\", 3000), \\\n", "    (\"Kumar\", \"Marketing\", 2000),\\\n", "    (\"Saif\", \"Sales\", 4100) \\\n", "  )\n", " \n", "columns= [\"employee_name\", \"department\", \"salary\"]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df = spark.createDataFrame(data = simpleData, schema = columns)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df.printSchema()\n", "df.show(truncate=False)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from pyspark.sql.window import Window\n", "from pyspark.sql.functions import row_number\n", "windowSpec  = Window.partitionBy(\"department\").orderBy(\"salary\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df.withColumn(\"row_number\",row_number().over(windowSpec)) \\\n", "    .show(truncate=False)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from pyspark.sql.functions import rank\n", "df.withColumn(\"rank\",rank().over(windowSpec)) \\\n", "    .show()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from pyspark.sql.functions import dense_rank\n", "df.withColumn(\"dense_rank\",dense_rank().over(windowSpec)) \\\n", "    .show()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from pyspark.sql.functions import percent_rank\n", "df.withColumn(\"percent_rank\",percent_rank().over(windowSpec)) \\\n", "    .show()\n", "    \n", "from pyspark.sql.functions import ntile\n", "df.withColumn(\"ntile\",ntile(2).over(windowSpec)) \\\n", "    .show()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from pyspark.sql.functions import cume_dist    \n", "df.withColumn(\"cume_dist\",cume_dist().over(windowSpec)) \\\n", "   .show()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from pyspark.sql.functions import lag    \n", "df.withColumn(\"lag\",lag(\"salary\",2).over(windowSpec)) \\\n", "      .show()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from pyspark.sql.functions import lead    \n", "df.withColumn(\"lead\",lead(\"salary\",2).over(windowSpec)) \\\n", "    .show()\n", "    \n", "windowSpecAgg  = Window.partitionBy(\"department\")\n", "from pyspark.sql.functions import col,avg,sum,min,max,row_number \n", "df.withColumn(\"row\",row_number().over(windowSpec)) \\\n", "  .withColumn(\"avg\", avg(col(\"salary\")).over(windowSpecAgg)) \\\n", "  .withColumn(\"sum\", sum(col(\"salary\")).over(windowSpecAgg)) \\\n", "  .withColumn(\"min\", min(col(\"salary\")).over(windowSpecAgg)) \\\n", "  .withColumn(\"max\", max(col(\"salary\")).over(windowSpecAgg)) \\\n", "  .where(col(\"row\")==1).select(\"department\",\"avg\",\"sum\",\"min\",\"max\") \\\n", "  .show()"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}